Prerequisites 先决条件
fundamental 基本的
especially 特别的
performance 性能
modified 修改
highly recommended 强推荐
be used to 被用来
used to 用于
be used on 被用在
a label with the value of this digit 一个带有这个数字的值的标签
tutorial 
validation 验证
lifting 提升
corresponding 相应的
stretched 拉伸的
independently, 独立的
whole 整个
interacting 交互
separate 单独的
a small amount of 少量的
symmetry 对称
prevent 防止
slightly positive 稍微积极
particular 特定的
corresponds to 对应于
get back to 回到
probability 概率
indicating 指示
a "stretched" array of pixel values 一个“拉伸”的像素值数组


To output one of the images, we reshape this long string of pixels into a 2-dimensional array, which is basically a grayscale image.
为了输出其中一个图像，我们将这一长串的像素重构为一个二维数组，基本上是灰度图像。


The corresponding labels are numbers between 0 and 9, describing which digit a given image is of.
对应的标签为0到9之间的数字，描述给定图像的数字。

TensorFlow does its heavy lifting outside Python. Therefore, 
instead of running every single operation independently, 
TensorFlow allows users to build a whole graph of interacting operations and then runs the workflow in a separate process at once.

在Python之外，TensorFlow执行了繁重的提升。
因此，TensorFlow不是独立运行每一个独立的操作，
而是允许用户构建一个交互操作的完整图形，然后在一个单独的过程中同时运行工作流。

For this problem we use zero padded convolutions so that the output is the same size as the input. 
Stride/step in this case is equal to 1.

In general, convolution layer is used to get the features of the data. 
In the case of digit recognition - a shape of each digit. 
It uses learnable kernels/filters each of which corresponds to one particular shape pattern. 
The number of the filter can differ for other problems.
对于这个问题，我们使用零附加的卷积，以便输出与输入的大小相同。
在这个例子中，迈步/步骤等于1。
一般来说，卷积层用于获取数据的特征。
在数字识别的情况下——每个数字的形状。
它使用可学习的内核/过滤器，每个过滤器对应一个特定的形状模式。
过滤器的数量会因其他问题而异。

Pooling is used for downsampling of the data. 
2x2 max-pooling splits the image into square 2-pixel blocks and only keeps maximum value for each of those blocks.
池化用于对数据的下行采样。将图像分割为2像素块，只对每个块保持最大值。

We'll get back to convolutions and pooling in more detail below.
我们将在下面详细介绍卷积和池。

The good thing about neural networks that 
any NN can be used as a layer in a large multilayer NN meaning that 
output of one can be used as input for another. 
This sequential approach can create very sophisticated NN with multiple layers. 
They are also called Deep Neural Networks..
神经网络的优点是，
任何NN都可以作为一个多层NN中的一个层使用，
这意味着一个可以作为另一个的输入。这个顺序的方法可以创建非常复杂的多层NN。
它们也被称为深层神经网络。

In this case, we use two convolution layers with pooling in between them, 
then densely connected layer followed by dropout and lastly readout layer.
带着dropout的全连接层，最后输出层

The first layer is a convolution, followed by max pooling. 
The convolution computes 32 features for each 5x5 patch. 
Its weight tensor has a shape of [5, 5, 1, 32]. 
The first two dimensions are the patch size, 
the next is the number of input channels (1 means that images are grayscale), 
and the last is the number of output channels. 
There is also a bias vector with a component for each output channel.
第一层是卷积，其次是最大池。
卷积计算每个5x5补丁的32个特性。
它的权重张量的形状是[5,5,1,32]。
前两个维度是patch的大小，
下一个是输入通道的数量(1表示图像是灰度)，
最后是输出通道的数量。
还有一个带有每个输出通道组件的偏置向量。

To apply the layer, we reshape the input data to a 4d tensor, 
with the first dimension corresponding to the number of images, 
second and third - to image width and height, 
and the final dimension - to the number of colour channels.
为了应用这一层，我们将输入数据重塑为一个4d张量，
第一个维度对应于图像的数量，
第二和第三个 对应于 图像的宽度和高度，
最后的维度 对应于 —颜色通道的数量。

After the convolution, pooling reduces the size of the output from 28x28 to 14x14.
在卷积之后，池化将输出的大小从28x28减小到14x14。

The second layer has 64 features for each 5x5 patch. 
Its weight tensor has a shape of [5, 5, 32, 64]. 
The first two dimensions are the patch size, 
the next is the number of input channels 
(32 channels correspond to 32 featured that we got from previous convolutional layer), 
and the last is the number of output channels. 
There is also a bias vector with a component for each output channel.
第二层为每个5x5补丁有64个特性。
它的重量张量有一个形状[5,5,2,64]。
前两个维度是patch的大小，下一个是输入通道的数量
(32个通道对应于我们从之前卷积层得到的32个通道)，
最后一个是输出通道的数量。
还有一个带有每个输出通道组件的偏置向量。

Because the image is down-sampled by pooling to 14x14 size 
second convolutional layer picks up more general characteristics of the images. 
因为图像是通过池化到14x14大小的第二个卷积层来下采样的，从而获得了更多的图像的一般特征
Filters cover more space of the picture. 
Therefore, it is adjusted for more generic features while the first layer finds smaller details.
过滤器覆盖了图片更多的空间。
因此，它被调整为更通用的特性，而第一层则寻找更小的细节

Now that the image size is reduced to 7x7, 
we add a fully-connected layer with 1024 neurones to 
allow processing on the entire image 
(each of the neurons of the fully connected layer is connected to 
all the activations/outpus of the previous layer)
现在图像的大小已经减小到7x7，
我们添加了一个与1024个神经元相连的全连通层，
以允许对整个图像进行处理
(全连通层的每一个神经元都与上一层的所有激活/ outpus连接)。

To prevent overfitting, we apply dropout before the readout layer.
Dropout removes some nodes from the network at each training stage. 
Each of the nodes is either kept in the network with 
probability keep_prob or dropped with probability 1 - keep_prob. 
After the training stage is over the nodes are returned to the NN with their original weights.
为了防止过度拟合，我们在读出层之前应用了dropout。
在每个训练阶段，dropout从网络中删除一些节点。
每一个节点要么保留在网络中，要么用概率keep_prob或以概率1 - keep_prob删除。
在训练阶段结束后，节点返回到NN的初始权重。

Finally, we add a softmax layer, the same one if we use just a simple softmax regression.
最后，我们添加一个softmax层，就如我们只使用一个简单的softmax回归。


To evaluate network performance we use cross-entropy and to minimise it ADAM optimiser is used.
ADAM optimiser is a gradient based optimization algorithm, 
based on adaptive estimates, it's more sophisticated than steepest gradient descent and is well suited for problems with large data or many parameters.
为了评估网络性能，我们使用交叉熵，并将其最小化。
ADAM optimiser是一种基于自适应估计的基于梯度的优化算法，
它比最陡峭的梯度下降更复杂，而且很适合于大数据或许多参数的问题。

To predict values from test data, 
highest probability is picked from "one-hot vector" indicating that chances of an image being one of the digits are highest.
为了从测试数据中预测值，最高概率被从“一个热向量”中挑选出来，这表明一个图像的概率是最高的。

Ideally, we should use all data for every step of the training, 
but that's expensive. So, instead, we use small "batches" of random data.
This method is called stochastic training. 
It is cheaper, faster and gives much of the same result.
理想情况下，我们应该在培训的每一步都使用所有的数据，
但这很昂贵。因此，我们使用随机数据的一小部分。
这种方法被称为随机训练。
它更便宜，更快，并提供了许多相同的结果。

Now when all operations for every variable are defined in TensorFlow graph 
all computations will be performed outside Python environment.
现在，当所有变量的操作都在TensorFlow图中定义时，
所有计算都将在Python环境之外执行。

Each step of the loop, we get a "batch" of data points from the training set and 
feed it to the graph to replace the placeholders. In this case, it's: x, y and dropout.
Also, once in a while, we check training accuracy on an upcoming "batch".
On the local environment, we recommend saving training progress, 
so it can be recovered for further training, debugging or evaluation.
在循环的每一步中，我们从训练集得到一个“批”数据点，
并将其提供给图表以替换占位符。在这种情况下，它是:x,y和dropout。
同时，我们在一段时间内检查了即将到来的“批量”的训练准确性。
在本地的环境下，我们建议将培训进度保存，
以便可以恢复培训、调试或评估。

After training is done, it's good to check accuracy on data that wasn't used in training.
在训练完成后，检查在训练中未使用的数据的准确性是很好的。

When, we're happy with the outcome, we read test data from test.csv 
an dpredict labels for provided images.
Test data contains only images and labels are missing. 
Otherwise, the structure is similar to training data.
Predicted labels are stored into CSV file for future submission.
当我们对结果感到满意时，我们从测试中读取测试数据。为提供的图像提供csv和预测标签。
测试数据只包含图像和标签。否则，结构与训练数据类似。
预测标签存储到CSV文件中，以备将来提交。

As it was mentioned before, it is good to output some variables for a better understanding of the process.
Here we pull an output of the first convolution layer from TensorFlow graph. 
32 features are transformed into an image grid, 
and it's quite interesting to see how filters picked by NN outline characteristics of different digits.
正如前面提到的，输出一些变量以更好地理解流程是很好的。
在这里，我们从肌腱流图拉出第一个卷积层的输出。
32个特征被转换成一个图像网格，
很有趣的是，看看NN的不同数字的特征是如何选择的
